{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyexpat import model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import locale\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import preprocessing\n",
    "import locale\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from deepforest import CascadeForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,input_shape = (320,320,9), kernel_size = (7,7), strides = 2, padding = 'valid', activation = 'relu'))\n",
    "    model.add(AveragePooling2D(pool_size = 2))\n",
    "    model.add(Conv2D(64, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), strides  = 1, activation = 'relu', padding  = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size = (1,1), strides  = 1, activation = 'relu', padding  = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), strides  = 1, activation = 'relu', padding  = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size = (1,1), strides  = 1, activation = 'relu', padding  = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), strides  = 1, activation = 'relu', padding  = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size = (1,1), strides  = 1, activation = 'relu', padding  = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), strides  = 1, activation = 'relu', padding  = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size = (1,1), strides  = 1, activation = 'relu', padding  = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), strides  = 1, activation = 'relu', padding  = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size = (1,1), strides  = 1, activation = 'relu', padding  = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), strides  = 1, activation = 'relu', padding  = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size = (1,1), strides  = 1, activation = 'relu', padding  = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size = (1,1), strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=(1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(128, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=(1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(128, kernel_size = (1,1), strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=(1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(128, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=(1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(128, kernel_size = (1,1), strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=(1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(128, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=(1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(128, kernel_size = (1,1), strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=(1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(128, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=(1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size=(1,1),strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(1024, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size=(1,1),strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2048, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size=(1,1),strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2048, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size=(1,1),strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2048, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size=(1,1),strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2048, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size=(1,1),strides = 2, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2048, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size=(1,1),strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512,kernel_size = (3,3), strides = 1,activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2048, kernel_size = (1,1), strides = 1, activation = 'relu', padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D(pool_size = 2, padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "A00001.png\n",
      "A00002.png\n",
      "A00003.png\n",
      "A00004.png\n",
      "A00005.png\n",
      "A00006.png\n",
      "A00007.png\n",
      "A00008.png\n",
      "A00009.png\n",
      "A00010.png\n",
      "A00011.png\n",
      "A00012.png\n",
      "A00013.png\n",
      "A00014.png\n",
      "A00015.png\n",
      "A00016.png\n",
      "A00017.png\n",
      "A00018.png\n",
      "A00019.png\n",
      "A00020.png\n",
      "A00021.png\n",
      "A00022.png\n",
      "A00023.png\n",
      "A00024.png\n",
      "A00025.png\n",
      "A00026.png\n",
      "A00027.png\n",
      "A00028.png\n",
      "A00029.png\n",
      "A00030.png\n",
      "A00031.png\n",
      "A00032.png\n",
      "A00033.png\n",
      "A00034.png\n",
      "A00035.png\n",
      "A00036.png\n",
      "A00037.png\n",
      "A00038.png\n",
      "A00039.png\n",
      "A00040.png\n",
      "A00041.png\n",
      "A00042.png\n",
      "A00043.png\n",
      "A00044.png\n",
      "A00045.png\n",
      "A00046.png\n",
      "A00047.png\n",
      "A00048.png\n",
      "A00049.png\n",
      "A00050.png\n",
      "A00051.png\n",
      "A00052.png\n",
      "A00053.png\n",
      "A00054.png\n",
      "A00055.png\n",
      "A00056.png\n",
      "A00057.png\n",
      "A00058.png\n",
      "A00059.png\n",
      "A00060.png\n",
      "A00061.png\n",
      "A00062.png\n",
      "A00063.png\n",
      "A00064.png\n",
      "A00065.png\n",
      "A00066.png\n",
      "A00067.png\n",
      "A00068.png\n",
      "A00069.png\n",
      "A00070.png\n",
      "A00071.png\n",
      "A00072.png\n",
      "A00073.png\n",
      "A00074.png\n",
      "A00075.png\n",
      "A00076.png\n",
      "A00077.png\n",
      "A00078.png\n",
      "A00079.png\n",
      "A00080.png\n",
      "A00081.png\n",
      "A00082.png\n",
      "A00083.png\n",
      "A00084.png\n",
      "A00085.png\n",
      "A00086.png\n",
      "A00087.png\n",
      "A00088.png\n",
      "A00089.png\n",
      "A00090.png\n",
      "A00091.png\n",
      "A00092.png\n",
      "A00093.png\n",
      "A00094.png\n",
      "A00095.png\n",
      "A00096.png\n",
      "A00097.png\n",
      "A00098.png\n",
      "A00099.png\n",
      "A00100.png\n",
      "A00101.png\n",
      "A00102.png\n",
      "A00103.png\n",
      "A00104.png\n",
      "A00105.png\n",
      "A00106.png\n",
      "A00107.png\n",
      "A00108.png\n",
      "A00109.png\n",
      "A00110.png\n",
      "A00111.png\n",
      "A00112.png\n",
      "A00113.png\n",
      "A00114.png\n",
      "A00115.png\n",
      "A00116.png\n",
      "A00117.png\n",
      "A00118.png\n",
      "A00119.png\n",
      "A00120.png\n",
      "A00121.png\n",
      "A00122.png\n",
      "A00123.png\n",
      "A00124.png\n",
      "A00125.png\n",
      "A00126.png\n",
      "A00127.png\n",
      "A00128.png\n",
      "A00129.png\n",
      "A00130.png\n",
      "A00131.png\n",
      "A00132.png\n",
      "A00133.png\n",
      "A00134.png\n",
      "A00135.png\n",
      "A00136.png\n",
      "A00137.png\n",
      "A00138.png\n",
      "A00139.png\n",
      "A00140.png\n",
      "A00141.png\n",
      "A00142.png\n",
      "A00143.png\n",
      "A00144.png\n",
      "A00145.png\n",
      "A00146.png\n",
      "A00147.png\n",
      "A00148.png\n",
      "A00149.png\n",
      "A00150.png\n",
      "A00151.png\n",
      "A00152.png\n",
      "A00153.png\n",
      "A00154.png\n",
      "A00155.png\n",
      "A00156.png\n",
      "A00157.png\n",
      "A00158.png\n",
      "A00159.png\n",
      "A00160.png\n",
      "A00161.png\n",
      "A00162.png\n",
      "A00163.png\n",
      "A00164.png\n",
      "A00165.png\n",
      "A00166.png\n",
      "A00167.png\n",
      "A00168.png\n",
      "A00169.png\n",
      "A00170.png\n",
      "A00171.png\n",
      "A00172.png\n",
      "A00173.png\n",
      "A00174.png\n",
      "A00175.png\n",
      "A00176.png\n",
      "A00177.png\n",
      "A00178.png\n",
      "A00179.png\n",
      "A00180.png\n",
      "A00181.png\n",
      "A00182.png\n",
      "A00183.png\n",
      "A00184.png\n",
      "A00185.png\n",
      "A00186.png\n",
      "A00187.png\n",
      "A00188.png\n",
      "A00189.png\n",
      "A00190.png\n",
      "A00191.png\n",
      "A00192.png\n",
      "A00193.png\n",
      "A00194.png\n",
      "A00195.png\n",
      "A00196.png\n",
      "A00197.png\n",
      "A00198.png\n",
      "A00199.png\n",
      "A00200.png\n",
      "A00201.png\n",
      "A00202.png\n",
      "A00203.png\n",
      "A00204.png\n",
      "A00205.png\n",
      "A00206.png\n",
      "A00207.png\n",
      "A00208.png\n",
      "A00209.png\n",
      "A00210.png\n",
      "A00211.png\n",
      "A00212.png\n",
      "A00213.png\n",
      "A00214.png\n",
      "A00215.png\n",
      "A00216.png\n",
      "A00217.png\n",
      "A00218.png\n",
      "A00219.png\n",
      "A00220.png\n",
      "A00221.png\n",
      "A00222.png\n",
      "A00223.png\n",
      "A00224.png\n",
      "A00225.png\n",
      "A00226.png\n",
      "A00227.png\n",
      "A00228.png\n",
      "A00229.png\n",
      "A00230.png\n",
      "A00231.png\n",
      "A00232.png\n",
      "A00233.png\n",
      "A00234.png\n",
      "A00235.png\n",
      "A00236.png\n",
      "A00237.png\n",
      "A00238.png\n",
      "A00239.png\n",
      "A00240.png\n",
      "A00241.png\n",
      "A00242.png\n",
      "A00243.png\n",
      "A00244.png\n",
      "A00245.png\n",
      "A00246.png\n",
      "A00247.png\n",
      "A00248.png\n",
      "A00249.png\n",
      "A00250.png\n",
      "A00251.png\n",
      "A00252.png\n",
      "A00253.png\n",
      "A00254.png\n",
      "A00255.png\n",
      "A00256.png\n",
      "A00257.png\n",
      "A00258.png\n",
      "A00259.png\n",
      "A00260.png\n",
      "A00261.png\n",
      "A00262.png\n",
      "A00263.png\n",
      "A00264.png\n",
      "A00265.png\n",
      "A00266.png\n",
      "A00267.png\n",
      "A00268.png\n",
      "A00269.png\n",
      "A00270.png\n",
      "A00271.png\n",
      "A00272.png\n",
      "A00273.png\n",
      "A00274.png\n",
      "A00275.png\n",
      "A00276.png\n",
      "A00277.png\n",
      "A00278.png\n",
      "A00279.png\n",
      "A00280.png\n",
      "A00281.png\n",
      "A00282.png\n",
      "A00283.png\n",
      "A00284.png\n",
      "A00285.png\n",
      "A00286.png\n",
      "A00287.png\n",
      "A00288.png\n",
      "A00289.png\n",
      "A00290.png\n",
      "A00291.png\n",
      "A00292.png\n",
      "A00293.png\n",
      "A00294.png\n",
      "A00295.png\n",
      "A00296.png\n",
      "A00297.png\n",
      "A00298.png\n",
      "A00299.png\n",
      "A00300.png\n",
      "A00301.png\n",
      "A00302.png\n",
      "A00303.png\n",
      "A00304.png\n",
      "A00305.png\n",
      "A00306.png\n",
      "A00307.png\n",
      "A00308.png\n",
      "A00309.png\n",
      "A00310.png\n",
      "A00311.png\n",
      "A00312.png\n",
      "A00313.png\n",
      "A00314.png\n",
      "A00315.png\n",
      "A00316.png\n",
      "A00317.png\n",
      "A00318.png\n",
      "A00319.png\n",
      "A00320.png\n",
      "A00321.png\n",
      "A00322.png\n",
      "A00323.png\n",
      "A00324.png\n",
      "A00325.png\n",
      "A00326.png\n",
      "A00327.png\n",
      "A00328.png\n",
      "A00329.png\n",
      "A00330.png\n",
      "A00331.png\n",
      "A00332.png\n",
      "A00333.png\n",
      "A00334.png\n",
      "A00335.png\n",
      "A00336.png\n",
      "A00337.png\n",
      "A00338.png\n",
      "A00339.png\n",
      "A00340.png\n",
      "A00341.png\n",
      "A00342.png\n",
      "A00343.png\n",
      "A00344.png\n",
      "A00345.png\n",
      "A00346.png\n",
      "A00347.png\n",
      "A00348.png\n",
      "A00349.png\n",
      "A00350.png\n",
      "A00351.png\n",
      "A00352.png\n",
      "A00353.png\n",
      "A00354.png\n",
      "A00355.png\n",
      "A00356.png\n",
      "A00357.png\n",
      "A00358.png\n",
      "A00359.png\n",
      "A00360.png\n",
      "A00361.png\n",
      "A00362.png\n",
      "A00363.png\n",
      "A00364.png\n",
      "A00365.png\n",
      "A00366.png\n",
      "A00367.png\n",
      "A00368.png\n",
      "A00369.png\n",
      "A00370.png\n",
      "A00371.png\n",
      "A00372.png\n",
      "A00373.png\n",
      "A00374.png\n",
      "A00375.png\n",
      "A00376.png\n",
      "A00377.png\n",
      "A00378.png\n",
      "A00379.png\n",
      "A00380.png\n",
      "A00381.png\n",
      "A00382.png\n",
      "A00383.png\n",
      "A00384.png\n",
      "A00385.png\n",
      "A00386.png\n",
      "A00387.png\n",
      "A00388.png\n",
      "A00389.png\n",
      "A00390.png\n",
      "A00391.png\n",
      "A00392.png\n",
      "A00393.png\n",
      "A00394.png\n",
      "A00395.png\n",
      "A00396.png\n",
      "A00397.png\n",
      "A00398.png\n",
      "A00399.png\n",
      "A00400.png\n",
      "A00401.png\n",
      "A00402.png\n",
      "A00403.png\n",
      "A00404.png\n",
      "A00405.png\n",
      "A00406.png\n",
      "A00407.png\n",
      "A00408.png\n",
      "A00409.png\n",
      "A00410.png\n",
      "A00411.png\n",
      "A00412.png\n",
      "A00413.png\n",
      "A00414.png\n",
      "A00415.png\n",
      "A00416.png\n",
      "A00417.png\n",
      "A00418.png\n",
      "A00419.png\n",
      "A00420.png\n",
      "A00421.png\n",
      "A00422.png\n",
      "A00423.png\n",
      "A00424.png\n",
      "A00425.png\n",
      "A00426.png\n",
      "A00427.png\n",
      "A00428.png\n",
      "A00429.png\n",
      "A00430.png\n",
      "A00431.png\n",
      "A00432.png\n",
      "A00433.png\n",
      "A00434.png\n",
      "A00435.png\n",
      "A00436.png\n",
      "A00437.png\n",
      "A00438.png\n",
      "A00439.png\n",
      "A00440.png\n",
      "A00441.png\n",
      "A00442.png\n",
      "A00443.png\n",
      "A00444.png\n",
      "A00445.png\n",
      "A00446.png\n",
      "A00447.png\n",
      "A00448.png\n",
      "A00449.png\n",
      "A00450.png\n",
      "A00451.png\n",
      "A00452.png\n",
      "A00453.png\n",
      "A00454.png\n",
      "A00455.png\n",
      "A00456.png\n",
      "A00457.png\n",
      "A00458.png\n",
      "A00459.png\n",
      "A00460.png\n",
      "A00461.png\n",
      "A00462.png\n",
      "A00463.png\n",
      "A00464.png\n",
      "A00465.png\n",
      "A00466.png\n",
      "A00467.png\n",
      "A00468.png\n",
      "A00469.png\n",
      "A00470.png\n",
      "A00471.png\n",
      "A00472.png\n",
      "A00473.png\n",
      "A00474.png\n",
      "A00475.png\n",
      "A00476.png\n",
      "A00477.png\n",
      "A00478.png\n",
      "A00479.png\n",
      "A00480.png\n",
      "A00481.png\n",
      "A00482.png\n",
      "A00483.png\n",
      "A00484.png\n",
      "A00485.png\n",
      "A00486.png\n",
      "A00487.png\n",
      "A00488.png\n",
      "A00489.png\n",
      "A00490.png\n",
      "A00491.png\n",
      "A00492.png\n",
      "A00493.png\n",
      "A00494.png\n",
      "A00495.png\n",
      "A00496.png\n",
      "A00497.png\n",
      "A00498.png\n",
      "A00499.png\n",
      "A00500.png\n",
      "B00001.png\n",
      "B00002.png\n",
      "B00003.png\n",
      "B00004.png\n",
      "B00005.png\n",
      "B00006.png\n",
      "B00007.png\n",
      "B00008.png\n",
      "B00009.png\n",
      "B00010.png\n",
      "B00011.png\n",
      "B00012.png\n",
      "B00013.png\n",
      "B00014.png\n",
      "B00015.png\n",
      "B00016.png\n",
      "B00017.png\n",
      "B00018.png\n",
      "B00019.png\n",
      "B00020.png\n",
      "B00021.png\n",
      "B00022.png\n",
      "B00023.png\n",
      "B00024.png\n",
      "B00025.png\n",
      "B00026.png\n",
      "B00027.png\n",
      "B00028.png\n",
      "B00029.png\n",
      "B00030.png\n",
      "B00031.png\n",
      "B00032.png\n",
      "B00033.png\n",
      "B00034.png\n",
      "B00035.png\n",
      "B00036.png\n",
      "B00037.png\n",
      "B00038.png\n",
      "B00039.png\n",
      "B00040.png\n",
      "B00041.png\n",
      "B00042.png\n",
      "B00043.png\n",
      "B00044.png\n",
      "B00045.png\n",
      "B00046.png\n",
      "B00047.png\n",
      "B00048.png\n",
      "B00049.png\n",
      "B00050.png\n",
      "B00051.png\n",
      "B00052.png\n",
      "B00053.png\n",
      "B00054.png\n",
      "B00055.png\n",
      "B00056.png\n",
      "B00057.png\n",
      "B00058.png\n",
      "B00059.png\n",
      "B00060.png\n",
      "B00061.png\n",
      "B00062.png\n",
      "B00063.png\n",
      "B00064.png\n",
      "B00065.png\n",
      "B00066.png\n",
      "B00067.png\n",
      "B00068.png\n",
      "B00069.png\n",
      "B00070.png\n",
      "B00071.png\n",
      "B00072.png\n",
      "B00073.png\n",
      "B00074.png\n",
      "B00075.png\n",
      "B00076.png\n",
      "B00077.png\n",
      "B00078.png\n",
      "B00079.png\n",
      "B00080.png\n",
      "B00081.png\n",
      "B00082.png\n",
      "B00083.png\n",
      "B00084.png\n",
      "B00085.png\n",
      "B00086.png\n",
      "B00087.png\n",
      "B00088.png\n",
      "B00089.png\n",
      "B00090.png\n",
      "B00091.png\n",
      "B00092.png\n",
      "B00093.png\n",
      "B00094.png\n",
      "B00095.png\n",
      "B00096.png\n",
      "B00097.png\n",
      "B00098.png\n",
      "B00099.png\n",
      "B00100.png\n",
      "B00101.png\n",
      "B00102.png\n",
      "B00103.png\n",
      "B00104.png\n",
      "B00105.png\n",
      "B00106.png\n",
      "B00107.png\n",
      "B00108.png\n",
      "B00109.png\n",
      "B00110.png\n",
      "B00111.png\n",
      "B00112.png\n",
      "B00113.png\n",
      "B00114.png\n",
      "B00115.png\n",
      "B00116.png\n",
      "B00117.png\n",
      "B00118.png\n",
      "B00119.png\n",
      "B00120.png\n",
      "B00121.png\n",
      "B00122.png\n",
      "B00123.png\n",
      "B00124.png\n",
      "B00125.png\n",
      "B00126.png\n",
      "B00127.png\n",
      "B00128.png\n",
      "B00129.png\n",
      "B00130.png\n",
      "B00131.png\n",
      "B00132.png\n",
      "B00133.png\n",
      "B00134.png\n",
      "B00135.png\n",
      "B00136.png\n",
      "B00137.png\n",
      "B00138.png\n",
      "B00139.png\n",
      "B00140.png\n",
      "B00141.png\n",
      "B00142.png\n",
      "B00143.png\n",
      "B00144.png\n",
      "B00145.png\n",
      "B00146.png\n",
      "B00147.png\n",
      "B00148.png\n",
      "B00149.png\n",
      "B00150.png\n",
      "B00151.png\n",
      "B00152.png\n",
      "B00153.png\n",
      "B00154.png\n",
      "B00155.png\n",
      "B00156.png\n",
      "B00157.png\n",
      "B00158.png\n",
      "B00159.png\n",
      "B00160.png\n",
      "B00161.png\n",
      "B00162.png\n",
      "B00163.png\n",
      "B00164.png\n",
      "B00165.png\n",
      "B00166.png\n",
      "B00167.png\n",
      "B00168.png\n",
      "B00169.png\n",
      "B00170.png\n",
      "B00171.png\n",
      "B00172.png\n",
      "B00173.png\n",
      "B00174.png\n",
      "B00175.png\n",
      "B00176.png\n",
      "B00177.png\n",
      "B00178.png\n",
      "B00179.png\n",
      "B00180.png\n",
      "B00181.png\n",
      "B00182.png\n",
      "B00183.png\n",
      "B00184.png\n",
      "B00185.png\n",
      "B00186.png\n",
      "B00187.png\n",
      "B00188.png\n",
      "B00189.png\n",
      "B00190.png\n",
      "B00191.png\n",
      "B00192.png\n",
      "B00193.png\n",
      "B00194.png\n",
      "B00195.png\n",
      "B00196.png\n",
      "B00197.png\n",
      "B00198.png\n",
      "B00199.png\n",
      "B00200.png\n",
      "B00201.png\n",
      "B00202.png\n",
      "B00203.png\n",
      "B00204.png\n",
      "B00205.png\n",
      "B00206.png\n",
      "B00207.png\n",
      "B00208.png\n",
      "B00209.png\n",
      "B00210.png\n",
      "B00211.png\n",
      "B00212.png\n",
      "B00213.png\n",
      "B00214.png\n",
      "B00215.png\n",
      "B00216.png\n",
      "B00217.png\n",
      "B00218.png\n",
      "B00219.png\n",
      "B00220.png\n",
      "B00221.png\n",
      "B00222.png\n",
      "B00223.png\n",
      "B00224.png\n",
      "B00225.png\n",
      "B00226.png\n",
      "B00227.png\n",
      "B00228.png\n",
      "B00229.png\n",
      "B00230.png\n",
      "B00231.png\n",
      "B00232.png\n",
      "B00233.png\n",
      "B00234.png\n",
      "B00235.png\n",
      "B00236.png\n",
      "B00237.png\n",
      "B00238.png\n",
      "B00239.png\n",
      "B00240.png\n",
      "B00241.png\n",
      "B00242.png\n",
      "B00243.png\n",
      "B00244.png\n",
      "B00245.png\n",
      "B00246.png\n",
      "B00247.png\n",
      "B00248.png\n",
      "B00249.png\n",
      "B00250.png\n",
      "B00251.png\n",
      "B00252.png\n",
      "B00253.png\n",
      "B00254.png\n",
      "B00255.png\n",
      "B00256.png\n",
      "B00257.png\n",
      "B00258.png\n",
      "B00259.png\n",
      "B00260.png\n",
      "B00261.png\n",
      "B00262.png\n",
      "B00263.png\n",
      "B00264.png\n",
      "B00265.png\n",
      "B00266.png\n",
      "B00267.png\n",
      "B00268.png\n",
      "B00269.png\n",
      "B00270.png\n",
      "B00271.png\n",
      "B00272.png\n",
      "B00273.png\n",
      "B00274.png\n",
      "B00275.png\n",
      "B00276.png\n",
      "B00277.png\n",
      "B00278.png\n",
      "B00279.png\n",
      "B00280.png\n",
      "B00281.png\n",
      "B00282.png\n",
      "B00283.png\n",
      "B00284.png\n",
      "B00285.png\n",
      "B00286.png\n",
      "B00287.png\n",
      "B00288.png\n",
      "B00289.png\n",
      "B00290.png\n",
      "B00291.png\n",
      "B00292.png\n",
      "B00293.png\n",
      "B00294.png\n",
      "B00295.png\n",
      "B00296.png\n",
      "B00297.png\n",
      "B00298.png\n",
      "B00299.png\n",
      "B00300.png\n",
      "B00301.png\n",
      "B00302.png\n",
      "B00303.png\n",
      "B00304.png\n",
      "B00305.png\n",
      "B00306.png\n",
      "B00307.png\n",
      "B00308.png\n",
      "B00309.png\n",
      "B00310.png\n",
      "B00311.png\n",
      "B00312.png\n",
      "B00313.png\n",
      "B00314.png\n",
      "B00315.png\n",
      "B00316.png\n",
      "B00317.png\n",
      "B00318.png\n",
      "B00319.png\n",
      "B00320.png\n",
      "B00321.png\n",
      "B00322.png\n",
      "B00323.png\n",
      "B00324.png\n",
      "B00325.png\n",
      "B00326.png\n",
      "B00327.png\n",
      "B00328.png\n",
      "B00329.png\n",
      "B00330.png\n",
      "B00331.png\n",
      "B00332.png\n",
      "B00333.png\n",
      "B00334.png\n",
      "B00335.png\n",
      "B00336.png\n",
      "B00337.png\n",
      "B00338.png\n",
      "B00339.png\n",
      "B00340.png\n",
      "B00341.png\n",
      "B00342.png\n",
      "B00343.png\n",
      "B00344.png\n",
      "B00345.png\n",
      "B00346.png\n",
      "B00347.png\n",
      "B00348.png\n",
      "B00349.png\n",
      "B00350.png\n",
      "B00351.png\n",
      "B00352.png\n",
      "B00353.png\n",
      "B00354.png\n",
      "B00355.png\n",
      "B00356.png\n",
      "B00357.png\n",
      "B00358.png\n",
      "B00359.png\n",
      "B00360.png\n",
      "B00361.png\n",
      "B00362.png\n",
      "B00363.png\n",
      "B00364.png\n",
      "B00365.png\n",
      "B00366.png\n",
      "B00367.png\n",
      "B00368.png\n",
      "B00369.png\n",
      "B00370.png\n",
      "B00371.png\n",
      "B00372.png\n",
      "B00373.png\n",
      "B00374.png\n",
      "B00375.png\n",
      "B00376.png\n",
      "B00377.png\n",
      "B00378.png\n",
      "B00379.png\n",
      "B00380.png\n",
      "B00381.png\n",
      "B00382.png\n",
      "B00383.png\n",
      "B00384.png\n",
      "B00385.png\n",
      "B00386.png\n",
      "B00387.png\n",
      "B00388.png\n",
      "B00389.png\n",
      "B00390.png\n",
      "B00391.png\n",
      "B00392.png\n",
      "B00393.png\n",
      "B00394.png\n",
      "B00395.png\n",
      "B00396.png\n",
      "B00397.png\n",
      "B00398.png\n",
      "B00399.png\n",
      "B00400.png\n",
      "B00401.png\n",
      "B00402.png\n",
      "B00403.png\n",
      "B00404.png\n",
      "B00405.png\n",
      "B00406.png\n",
      "B00407.png\n",
      "B00408.png\n",
      "B00409.png\n",
      "B00410.png\n",
      "B00411.png\n",
      "B00412.png\n",
      "B00413.png\n",
      "B00414.png\n",
      "B00415.png\n",
      "B00416.png\n",
      "B00417.png\n",
      "B00418.png\n",
      "B00419.png\n",
      "B00420.png\n",
      "B00421.png\n",
      "B00422.png\n",
      "B00423.png\n",
      "B00424.png\n",
      "B00425.png\n",
      "B00426.png\n",
      "B00427.png\n",
      "B00428.png\n",
      "B00429.png\n",
      "B00430.png\n",
      "B00431.png\n",
      "B00432.png\n",
      "B00433.png\n",
      "B00434.png\n",
      "B00435.png\n",
      "B00436.png\n",
      "B00437.png\n",
      "B00438.png\n",
      "B00439.png\n",
      "B00440.png\n",
      "B00441.png\n",
      "B00442.png\n",
      "B00443.png\n",
      "B00444.png\n",
      "B00445.png\n",
      "B00446.png\n",
      "B00447.png\n",
      "B00448.png\n",
      "B00449.png\n",
      "B00450.png\n",
      "B00451.png\n",
      "B00452.png\n",
      "B00453.png\n",
      "B00454.png\n",
      "B00455.png\n",
      "B00456.png\n",
      "B00457.png\n",
      "B00458.png\n",
      "B00459.png\n",
      "B00460.png\n",
      "B00461.png\n",
      "B00462.png\n",
      "B00463.png\n",
      "B00464.png\n",
      "B00465.png\n",
      "B00466.png\n",
      "B00467.png\n",
      "B00468.png\n",
      "B00469.png\n",
      "B00470.png\n",
      "B00471.png\n",
      "B00472.png\n",
      "B00473.png\n",
      "B00474.png\n",
      "B00475.png\n",
      "B00476.png\n",
      "B00477.png\n",
      "B00478.png\n",
      "B00479.png\n",
      "B00480.png\n",
      "B00481.png\n",
      "B00482.png\n",
      "B00483.png\n",
      "B00484.png\n",
      "B00485.png\n",
      "B00486.png\n",
      "B00487.png\n",
      "B00488.png\n",
      "B00489.png\n",
      "B00490.png\n",
      "B00491.png\n",
      "B00492.png\n",
      "B00493.png\n",
      "B00494.png\n",
      "B00495.png\n",
      "B00496.png\n",
      "B00497.png\n",
      "B00498.png\n",
      "B00499.png\n",
      "B00500.png\n"
     ]
    }
   ],
   "source": [
    "gasdata = pd.read_csv(\"/Users/Eddie/Downloads/data5 (1).csv\")\n",
    "data_dir = '/Users/Eddie/Downloads/Data5'\n",
    "imagedata = sorted(os.listdir(data_dir))\n",
    "print(len(imagedata))\n",
    "X_data = []\n",
    "for image in imagedata:\n",
    "        print(image)\n",
    "        img = mpimg.imread('/Users/Eddie/Downloads/Data5/'+image)\n",
    "        img = img.reshape(320,320,9)\n",
    "        img = img/255.0\n",
    "        X_data.append(img)\n",
    "images = np.array(X_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MQ2       MQ7\n",
      "868  0.956140  0.792398\n",
      "663  0.859649  0.796784\n",
      "676  0.853801  0.796784\n",
      "117  0.590643  0.426901\n",
      "954  0.935673  0.736842\n",
      "..        ...       ...\n",
      "162  0.840643  0.627193\n",
      "672  0.859649  0.798246\n",
      "508  0.666667  0.321637\n",
      "970  0.934211  0.741228\n",
      "403  0.650585  0.447368\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "          MQ2       MQ7\n",
      "164  0.847059  0.635294\n",
      "634  0.804412  0.220588\n",
      "820  0.941176  0.797059\n",
      "221  0.604412  0.451471\n",
      "711  0.879412  0.800000\n",
      "..        ...       ...\n",
      "356  0.847059  0.597059\n",
      "276  0.717647  0.550000\n",
      "506  0.670588  0.213235\n",
      "712  0.879412  0.798529\n",
      "730  0.963235  0.832353\n",
      "\n",
      "[200 rows x 2 columns]\n",
      "868    1\n",
      "663    1\n",
      "676    1\n",
      "117    0\n",
      "954    1\n",
      "      ..\n",
      "162    0\n",
      "672    1\n",
      "508    1\n",
      "970    1\n",
      "403    0\n",
      "Name: Gas, Length: 800, dtype: int64\n",
      "164    0\n",
      "634    1\n",
      "820    1\n",
      "221    0\n",
      "711    1\n",
      "      ..\n",
      "356    0\n",
      "276    0\n",
      "506    1\n",
      "712    1\n",
      "730    1\n",
      "Name: Gas, Length: 200, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "split = train_test_split(gasdata, images, test_size=0.2)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n",
    "trainy = trainAttrX[\"Gas\"]\n",
    "testy = testAttrX[\"Gas\"]\n",
    "trainAttrX = trainAttrX.drop(columns=['Gas'])\n",
    "testAttrX = testAttrX.drop(columns = ['Gas'])\n",
    "trainAttrX= (trainAttrX - np.min(trainAttrX)) / (np.max(trainAttrX) - np.min(trainAttrX))\n",
    "testAttrX = (testAttrX - np.min(testAttrX)) / (np.max(testAttrX) - np.min(testAttrX))\n",
    "print(trainAttrX)\n",
    "print(testAttrX)\n",
    "print(trainy)\n",
    "print(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164    0\n",
      "634    1\n",
      "820    1\n",
      "221    0\n",
      "711    1\n",
      "      ..\n",
      "356    0\n",
      "276    0\n",
      "506    1\n",
      "712    1\n",
      "730    1\n",
      "Name: Gas, Length: 200, dtype: int64\n",
      "[0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(testy)\n",
    "newtesty = []\n",
    "for value in testy:\n",
    "    value = float(value)\n",
    "    newtesty.append(value)\n",
    "print(newtesty)\n",
    "newtrainy = []\n",
    "for value in trainy:\n",
    "    value = float(value)\n",
    "    newtrainy.append(value)\n",
    "\n",
    "print(newtrainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "25/25 [==============================] - 27s 969ms/step\n",
      "(800, 2048)\n"
     ]
    }
   ],
   "source": [
    "cnn = create_cnn()\n",
    "X = cnn.predict(trainImagesX)\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4720514e-30 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  3.4933059e-29 6.6275228e-29]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  3.2457715e-29 5.8359950e-29]\n",
      " [2.1706748e-30 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  3.0329289e-29 5.7165633e-29]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  3.5438730e-29 6.3126766e-29]\n",
      " [7.9781671e-30 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  3.6110642e-29 6.3626376e-29]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  2.1086214e-29 4.8774164e-29]]\n",
      "\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "[0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n",
      "(800, 9216)\n",
      "[[0.95614035 0.79239766 0.         ... 0.         0.         0.        ]\n",
      " [0.85964912 0.79678363 0.         ... 0.         0.         0.        ]\n",
      " [0.85380117 0.79678363 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.66666667 0.32163743 0.         ... 0.         0.         0.        ]\n",
      " [0.93421053 0.74122807 0.         ... 0.         0.         0.        ]\n",
      " [0.6505848  0.44736842 0.         ... 0.         0.         0.        ]]\n",
      "[[1.4720514e-30 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [2.1706748e-30 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [7.9781671e-30 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "(800, 11264)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "print(X)\n",
    "trainAttrX = np.array(trainAttrX)\n",
    "trainAttrX.resize(800,9216)\n",
    "newtrainy=np.array(newtrainy)\n",
    "newtrainy = newtrainy.reshape(-1,1)\n",
    "print()\n",
    "print(newtrainy)\n",
    "print(newtesty)\n",
    "print(trainAttrX.shape)\n",
    "print(trainAttrX)\n",
    "traindata = concatenate([X,trainAttrX])\n",
    "traindata = np.array(traindata)\n",
    "traindata.reshape(-1,1)\n",
    "print(traindata)\n",
    "print(traindata.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-29 21:09:07.154] Start to fit the model:\n",
      "[2023-12-29 21:09:07.154] Fitting cascade layer = 0 \n",
      "[2023-12-29 21:12:25.804] layer = 0  | Val Acc = 87.375 % | Elapsed = 198.649 s\n",
      "[2023-12-29 21:12:25.952] Fitting cascade layer = 1 \n",
      "[2023-12-29 21:13:22.736] layer = 1  | Val Acc = 88.375 % | Elapsed = 56.783 s\n",
      "[2023-12-29 21:13:22.870] Fitting cascade layer = 2 \n",
      "[2023-12-29 21:14:17.736] layer = 2  | Val Acc = 88.750 % | Elapsed = 54.866 s\n",
      "[2023-12-29 21:14:17.872] Fitting cascade layer = 3 \n",
      "[2023-12-29 21:15:13.852] layer = 3  | Val Acc = 88.000 % | Elapsed = 55.980 s\n",
      "[2023-12-29 21:15:13.852] Early stopping counter: 1 out of 2\n",
      "[2023-12-29 21:15:13.972] Fitting cascade layer = 4 \n",
      "[2023-12-29 21:16:10.288] layer = 4  | Val Acc = 87.875 % | Elapsed = 56.316 s\n",
      "[2023-12-29 21:16:10.288] Early stopping counter: 2 out of 2\n",
      "[2023-12-29 21:16:10.288] Handling early stopping\n",
      "[2023-12-29 21:16:10.298] The optimal number of layers: 3\n"
     ]
    }
   ],
   "source": [
    "model = CascadeForestClassifier(n_estimators=50,random_state=1)\n",
    "model.fit(traindata,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 7s 827ms/step\n",
      "(200, 2048)\n",
      "(200, 2)\n"
     ]
    }
   ],
   "source": [
    "cows = cnn.predict(testImagesX)\n",
    "cows = np.array(cows)\n",
    "# print(cows)\n",
    "print(cows.shape)\n",
    "print(testAttrX.shape)\n",
    "# testAttrX.resize(122,2)\n",
    "# print(gasdata)\n",
    "# print(gasdata.shape)\n",
    "test = concatenate([cows,testAttrX])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-29 21:16:17.871] Start to evalute the model:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The binner was fitted with 11264 features but 2050 features got passed to `transform`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m performance \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m cows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(testy)\n\u001b[0;32m      3\u001b[0m performance\u001b[38;5;241m.\u001b[39mround()\n",
      "File \u001b[1;32mc:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deepforest\\cascade.py:1541\u001b[0m, in \u001b[0;36mCascadeForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;124;03mPredict class for X.\u001b[39;00m\n\u001b[0;32m   1527\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03m    The predicted classes.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X)\n\u001b[1;32m-> 1541\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1542\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_class_labels(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deepforest\\cascade.py:1465\u001b[0m, in \u001b[0;36mCascadeForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1462\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Start to evalute the model:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_utils\u001b[38;5;241m.\u001b[39mctime()))\n\u001b[0;32m   1464\u001b[0m binner_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_binner(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 1465\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bin_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinner_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1466\u001b[0m X_middle_test_ \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39minit_array(X_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_aug_features_)\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers_):\n",
      "File \u001b[1;32mc:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deepforest\\cascade.py:696\u001b[0m, in \u001b[0;36mBaseCascadeForest._bin_data\u001b[1;34m(self, binner, X, is_training_data)\u001b[0m\n\u001b[0;32m    694\u001b[0m     X_binned \u001b[38;5;241m=\u001b[39m binner\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 696\u001b[0m     X_binned \u001b[38;5;241m=\u001b[39m \u001b[43mbinner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m     X_binned \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(X_binned)\n\u001b[0;32m    698\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deepforest\\_binner.py:161\u001b[0m, in \u001b[0;36mBinner.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bins_non_missing_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    157\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe binner was fitted with \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m features but \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m features got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m passed to `transform`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    162\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bins_non_missing_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, dtype\u001b[38;5;241m=\u001b[39mX_DTYPE, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m X_binned \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(X, dtype\u001b[38;5;241m=\u001b[39mX_BINNED_DTYPE, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The binner was fitted with 11264 features but 2050 features got passed to `transform`."
     ]
    }
   ],
   "source": [
    "performance = model.predict(test)\n",
    "cows = np.array(testy)\n",
    "performance.round()\n",
    "actual = []\n",
    "for value in performance: \n",
    "    print(value)\n",
    "    if(value>=0.5):\n",
    "            actual.append(1)\n",
    "    else:\n",
    "            actual.append(0)\n",
    "\n",
    "acutal = np.array(actual)\n",
    "print(actual)\n",
    "\n",
    "print(testy)\n",
    "precision = accuracy_score(testy, actual)\n",
    "print('Precision: %f' % precision)\n",
    "precision = precision_score(testy, actual)\n",
    "print('Precision: %f' % precision)\n",
    "recall = recall_score(testy, actual)\n",
    "print('Recall: %f' % recall)\n",
    "f1 = f1_score(testy, actual)\n",
    "print('F1 score: %f' % f1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
