{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyexpat import model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import locale\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import preprocessing\n",
    "import locale\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from deepforest import CascadeForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "def conv2d_bn(x, filters, num_row, num_col, padding=\"same\", strides=(1, 1)):\n",
    "\n",
    "  # adding conv-2d layer \n",
    "  # x = keras.layers.Conv2D(filters, (num_row, num_col),strides=strides,padding=padding)(x)\n",
    "  model.add(Conv2D(filters, (num_row, num_col),strides=strides,padding=padding))\n",
    "    \n",
    "  # adding batch normalization \n",
    "  x = keras.layers.BatchNormalization(scale=False)(x)\n",
    "    \n",
    "  # adding activation function \n",
    "  x = keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "  # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_a(x):    \n",
    "  \n",
    "  branch1x1 = conv2d_bn(x, 64, 1, 1)  \n",
    "\n",
    "  branch5x5 = conv2d_bn(x, 48, 1, 1)  \n",
    "  branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "  branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "  branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "  branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "  branch_pool = keras.layers.AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "  branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "  x = keras.layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool],axis = 3)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_a(x):  \n",
    "\n",
    "  branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding=\"valid\")\n",
    "\n",
    "  branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "  branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "  branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3, strides=(2, 2), padding=\"valid\")\n",
    "\n",
    "  branch_pool = keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "  x = keras.layers.concatenate([branch3x3, branch3x3dbl, branch_pool],axis = 3)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_b(x):\n",
    "\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = keras.layers.AveragePooling2D((3, 3), strides=(1, 1),padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = keras.layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],axis = 3)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_b(x): \n",
    "\n",
    "    branch3x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,strides=(2, 2), padding=\"valid\")\n",
    "\n",
    "    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "    branch7x7x3 = conv2d_bn( branch7x7x3, 192, 3, 3, strides=(2, 2), padding=\"valid\")\n",
    "\n",
    "    branch_pool = keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = keras.layers.concatenate([branch3x3, branch7x7x3, branch_pool],axis =3)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_c(x):        \n",
    "\n",
    "        branch1x1 = conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "        branch3x3 = conv2d_bn(x, 384, 1, 1)\n",
    "        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n",
    "        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n",
    "        branch3x3 = keras.layers.concatenate([branch3x3_1, branch3x3_2],axis = 3)\n",
    "\n",
    "        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n",
    "        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "        branch3x3dbl = keras.layers.concatenate([branch3x3dbl_1, branch3x3dbl_2],axis = 3)\n",
    "\n",
    "        branch_pool = keras.layers.AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = keras.layers.concatenate( [branch1x1, branch3x3, branch3x3dbl, branch_pool], axis = 3)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv2d_bn(keras.Input(shape=(320,320,9)), 32, 3, 3, strides=(2, 2), padding=\"valid\") \n",
    "# x = conv2d_bn(x, 32, 3, 3, padding=\"valid\")  \n",
    "# x = conv2d_bn(x, 64, 3, 3) \n",
    "# x = keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)   \n",
    "# x = conv2d_bn(x, 80, 1, 1, padding=\"valid\") \n",
    "# x = conv2d_bn(x, 192, 3, 3, padding=\"valid\")  \n",
    "# x = keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)  \n",
    "\n",
    "# x=inc_block_a(x) \n",
    "# x=inc_block_a(x) \n",
    "# x=inc_block_a(x) \n",
    "\n",
    "# x=reduction_block_a(x) \n",
    "\n",
    "#x=inc_block_b(x) \n",
    "#x=inc_block_b(x) \n",
    "# x=inc_block_b(x) \n",
    "# x=inc_block_b(x) \n",
    "# x =reduction_block_b(x) \n",
    "\n",
    "# x=inc_block_c(x) \n",
    "# x=inc_block_c(x) \n",
    "\n",
    "# model.add(Conv2D())\n",
    "# x = keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(x) \n",
    "# x= keras.layers.Flatten()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "gasdata = pd.read_csv(\"/Users/Eddie/Downloads/data5.csv\")\n",
    "data_dir = '/Users/Eddie/Downloads/Data5'\n",
    "imagedata = sorted(os.listdir(data_dir))\n",
    "print(len(imagedata))\n",
    "X_data = []\n",
    "for image in imagedata:\n",
    "        # print(image)\n",
    "        img = mpimg.imread('/Users/Eddie/Downloads/Data5/'+image)\n",
    "        img = img.reshape(320,320,9)\n",
    "        img = img/255.0\n",
    "        X_data.append(img)\n",
    "images = np.array(X_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = train_test_split(gasdata, images, test_size=0.2)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n",
    "trainy = trainAttrX[\"Gas\"]\n",
    "testy = testAttrX[\"Gas\"]\n",
    "trainAttrX = trainAttrX.drop(columns=['Gas'])\n",
    "testAttrX = testAttrX.drop(columns = ['Gas'])\n",
    "trainAttrX= (trainAttrX - np.min(trainAttrX)) / (np.max(trainAttrX) - np.min(trainAttrX))\n",
    "testAttrX = (testAttrX - np.min(testAttrX)) / (np.max(testAttrX) - np.min(testAttrX))\n",
    "# print(trainAttrX)\n",
    "# print(testAttrX)\n",
    "# print(trainy)\n",
    "# print(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529    1\n",
      "521    1\n",
      "19     0\n",
      "896    1\n",
      "368    0\n",
      "      ..\n",
      "986    1\n",
      "702    1\n",
      "345    0\n",
      "996    1\n",
      "833    1\n",
      "Name: Gas, Length: 200, dtype: int64\n",
      "[1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n",
      "[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(testy)\n",
    "newtesty = []\n",
    "for value in testy:\n",
    "    value = float(value)\n",
    "    newtesty.append(value)\n",
    "print(newtesty)\n",
    "newtrainy = []\n",
    "for value in trainy:\n",
    "    value = float(value)\n",
    "    newtrainy.append(value)\n",
    "\n",
    "print(newtrainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inception():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,kernel_size = (3,3),input_shape = (320,320,9),strides = (2,2),padding = 'valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,kernel_size=(3,3),strides=(1,1),padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    model.add(Conv2D(80,kernel_size=(1,1),strides=(1,1),padding='valid'))\n",
    "    model.add(Conv2D(192,kernel_size=(3,3),strides=(1,1),padding='valid'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides = (2,2)))\n",
    "    # print(model.summary())\n",
    "    \n",
    "    #block b\n",
    "    branch1x1=model\n",
    "    branch1x1.add(Conv2D(64,kernel_size=(1,1),strides=(1,1),padding='same'))\n",
    "    branch1x1.add(BatchNormalization())\n",
    "    #print(branch1x1.summary())\n",
    "    #print(\"branch1x1 layer output shape\")\n",
    "    #for layer in branch1x1.layers:\n",
    "    #   print(layer.output_shape)\n",
    "\n",
    "    branch7x7 = model\n",
    "    branch7x7.add(Conv2D(128,kernel_size=(1,1),strides=(1,1),padding='same'))\n",
    "    branch7x7.add(BatchNormalization())\n",
    "    branch7x7.add(Conv2D(128,kernel_size=(1,7),strides=(1,1),padding='same'))\n",
    "    branch7x7.add(BatchNormalization())\n",
    "    branch7x7.add(Conv2D(192,kernel_size=(7,1),strides=(1,1),padding='same'))\n",
    "    branch7x7.add(BatchNormalization())\n",
    "    #print(branch7x7.summary())\n",
    "    #print(\"branch7x7 layer output shape\")\n",
    "    #for layer in branch7x7.layers:\n",
    "    #  print(len(layer.output_shape))\n",
    "    #   print(layer.output_shape[0])\n",
    "\n",
    "    branch7x7dbl=model\n",
    "    branch7x7dbl.add(Conv2D(128,kernel_size=(1,1),strides=(1,1),padding='same'))\n",
    "    branch7x7dbl.add(BatchNormalization())\n",
    "    branch7x7dbl.add(Conv2D(128,kernel_size=(7,1),strides=(1,1),padding='same'))\n",
    "    branch7x7dbl.add(BatchNormalization())\n",
    "    branch7x7dbl.add(Conv2D(128,kernel_size=(1,7 ),strides=(1,1),padding='same'))\n",
    "    branch7x7dbl.add(BatchNormalization())\n",
    "    branch7x7dbl.add(Conv2D(192,kernel_size=(7,1),strides=(1,1),padding='same'))\n",
    "    branch7x7dbl.add(BatchNormalization())   \n",
    "    #print(branch7x7dbl.summary())\n",
    "    #print(\"branch7x7db1 layer output shape\")\n",
    "    #for layer in branch7x7dbl.layers:\n",
    "    #   print(len(layer.output_shape))\n",
    "\n",
    "    branch_pool = model\n",
    "    branch_pool.add(AveragePooling2D(pool_size=(3,3),strides=(1,1),padding='same'))\n",
    "    branch_pool.add(Conv2D(192,kernel_size=(1,1),strides=(1,1),padding='same'))\n",
    "    #print(branch_pool.summary())\n",
    "    #print(\"branch_pool layer output shape\")\n",
    "    #for layer in branch_pool.layers:\n",
    "    #   print(layer.output_shape)\n",
    "\n",
    "    cancatenate_input = keras.Input(shape=(320,320,9))\n",
    "    #cancatenate_output = keras.layers.Concatenate()([branch1x1(cancatenate_input),branch7x7(cancatenate_input)])\n",
    "    cancatenate_output = keras.layers.Concatenate()([branch1x1(cancatenate_input)])\n",
    "    cancatenate_model = Model(cancatenate_input, cancatenate_output)\n",
    "    cancatenate_model = Flatten()(cancatenate_model)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,kernel_size = (3,3),input_shape = (320,320,9),strides = (2,2),padding = 'valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=(3,3),strides=(1,1),padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "model.add(Conv2D(80,kernel_size=(1,1),strides=(1,1),padding='valid'))\n",
    "model.add(Conv2D(192,kernel_size=(3,3),strides=(1,1),padding='valid'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides = (2,2)))\n",
    "# print(model.summary())\n",
    "\n",
    "#block b\n",
    "branch1x1=model\n",
    "branch1x1.add(Conv2D(64,kernel_size=(1,1),strides=(1,1),padding='same'))\n",
    "branch1x1.add(BatchNormalization())\n",
    "#print(branch1x1.summary())\n",
    "#print(\"branch1x1 layer output shape\")\n",
    "#for layer in branch1x1.layers:\n",
    "#   print(layer.output_shape)\n",
    "\n",
    "branch7x7 = model\n",
    "branch7x7.add(Conv2D(128,kernel_size=(1,1),strides=(1,1),padding='same'))\n",
    "branch7x7.add(BatchNormalization())\n",
    "branch7x7.add(Conv2D(128,kernel_size=(1,7),strides=(1,1),padding='same'))\n",
    "branch7x7.add(BatchNormalization())\n",
    "branch7x7.add(Conv2D(192,kernel_size=(7,1),strides=(1,1),padding='same'))\n",
    "branch7x7.add(BatchNormalization())\n",
    "#print(branch7x7.summary())\n",
    "#print(\"branch7x7 layer output shape\")\n",
    "#for layer in branch7x7.layers:\n",
    "#  print(len(layer.output_shape))\n",
    "#   print(layer.output_shape[0])\n",
    "\n",
    "branch7x7dbl=model\n",
    "branch7x7dbl.add(Conv2D(128,kernel_size=(1,1),strides=(1,1),padding='same'))\n",
    "branch7x7dbl.add(BatchNormalization())\n",
    "branch7x7dbl.add(Conv2D(128,kernel_size=(7,1),strides=(1,1),padding='same'))\n",
    "branch7x7dbl.add(BatchNormalization())\n",
    "branch7x7dbl.add(Conv2D(128,kernel_size=(1,7 ),strides=(1,1),padding='same'))\n",
    "branch7x7dbl.add(BatchNormalization())\n",
    "branch7x7dbl.add(Conv2D(192,kernel_size=(7,1),strides=(1,1),padding='same'))\n",
    "branch7x7dbl.add(BatchNormalization())   \n",
    "#print(branch7x7dbl.summary())\n",
    "#print(\"branch7x7db1 layer output shape\")\n",
    "#for layer in branch7x7dbl.layers:\n",
    "#   print(len(layer.output_shape))\n",
    "\n",
    "branch_pool = model\n",
    "branch_pool.add(AveragePooling2D(pool_size=(3,3),strides=(1,1),padding='same'))\n",
    "branch_pool.add(Conv2D(192,kernel_size=(1,1),strides=(1,1),padding='same'))\n",
    "#print(branch_pool.summary())\n",
    "#print(\"branch_pool layer output shape\")\n",
    "#for layer in branch_pool.layers:\n",
    "#   print(layer.output_shape)\n",
    "\n",
    "cancatenate_input = keras.Input(shape=(320,320,9))\n",
    "#cancatenate_output = keras.layers.Concatenate()([branch1x1(cancatenate_input),branch7x7(cancatenate_input)])\n",
    "cancatenate_output = keras.layers.Concatenate()([branch1x1(cancatenate_input)])\n",
    "cancatenate_model = Model(cancatenate_input, cancatenate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 8s 307ms/step\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "X=model.predict(trainImagesX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 9216)\n",
      "[[-2.64727278e-04  1.26642437e-04  5.50581724e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 7.72156127e-05 -3.76027456e-05  3.78266559e-05 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 5.34997307e-05 -2.63210368e-05  1.20907374e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "(800, 563712)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "trainAttrX = np.array(trainAttrX)\n",
    "trainAttrX.resize(800,9216)\n",
    "X.resize(800,554496)\n",
    "newtrainy=np.array(newtrainy)\n",
    "newtrainy = newtrainy.reshape(-1,1)\n",
    "#print(X)\n",
    "#print(X.shape)\n",
    "#print(trainAttrX)\n",
    "print(trainAttrX.shape)\n",
    "traindata = concatenate([X,trainAttrX])\n",
    "traindata = np.array(traindata)\n",
    "traindata.reshape(-1,1)\n",
    "print(traindata)\n",
    "print(traindata.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = CascadeForestClassifier(n_estimators=50,random_state=1)\n",
    "mod.fit(traindata,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnpred = cnn.predict(trainImagesX)\n",
    "model.predict()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
